{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube Conversation Search\n",
    "## CS/INFO 4300 Language and Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer, LancasterStemmer\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from data_collection.util import *\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the data from the JSON file (Lemmatize, Flatten)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#male siri\n",
    "\n",
    "#Method to check for numbers\n",
    "def has_numbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a stemmer and lemmatizer for use with our captions\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "my_punct = set(string.punctuation)\n",
    "data_path = \"data/raw_comments_v2\"\n",
    "\n",
    "flat_comments_list = []\n",
    "flat_comments_map = {} #idx to vid_id\n",
    "idx = 0\n",
    "for vid_id in get_filenames(data_path):\n",
    "    video_data = json.load(open(data_path+\"/\"+vid_id+\".json\"))\n",
    "    if video_data is not None and 'comments' in video_data and video_data['comments'] is not None:\n",
    "        flat_comments = \"\"\n",
    "        for comment in video_data['comments']:\n",
    "            for word in word_tokenize(comment[\"text\"]):\n",
    "                if not word.startswith(\"+\") and word not in my_punct and not has_numbers(word):\n",
    "                    flat_comments += (lemmatizer.lemmatize(word.lower())+\" \")\n",
    "        if flat_comments != \"\":\n",
    "            flat_comments_list.append(flat_comments[:-1]) #add to list\n",
    "            flat_comments_map[idx] = vid_id               #add to subj\n",
    "            idx += 1                                      #increment i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build the document-term matrices\n",
    "\n",
    "Use `sklearn.feature_extraction.TfidfVectorizer`. Use unigrams only, disable idf, use `l1` normalization. \n",
    "\n",
    "Resulting matrices are `X_train` and `X_test`.\n",
    "\n",
    "**Note:** Remember to just `fit` on the training data. If a word occurs only in the test documents, our model should **not** be aware that the word exists, as we are trying to evaluate the performance on completely unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(ngram_range=(1,2), lowercase=True, strip_accents=\"unicode\", \n",
    "                      stop_words='english', use_idf=True, norm='l1', min_df=2, max_df=.9)\n",
    "comments_vectors = tfv.fit_transform(flat_comments_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5968, 740395)\n"
     ]
    }
   ],
   "source": [
    "print(comments_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trained_models/comments_tfv.pkl',\n",
       " 'trained_models/comments_tfv.pkl_01.npy',\n",
       " 'trained_models/comments_tfv.pkl_02.npy']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_data = False\n",
    "if save_data:\n",
    "    from sklearn.externals import joblib\n",
    "    from scipy import sparse, io\n",
    "    joblib.dump(tfv, r'trained_models/comments_tfv_v2.pkl')\n",
    "    io.mmwrite(\"commets_vectors_v2.mtx\", comments_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_data = False\n",
    "if load_data:\n",
    "    comments_vectors = io.mmread(\"commets_vectors.mtx\")\n",
    "    comments_vectors = newm.tocsr()\n",
    "    tfv = joblib.load('trained_models/comments_tfv_v2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Cosine Similarty Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(X, Y):\n",
    "    \"\"\"Calculates cosine Similarity of two vectors\n",
    "    !!!!MAKE SURE DOC IS SECOND ARGUMENT!!!!! (the Y)\n",
    "    =Inputs=\n",
    "    X : The question vectors (sparse or dense matrix)\n",
    "    Y : The passage vectors (sparse or dense matrix)\n",
    "    =Outputs=\n",
    "     Consine Similarity (float)\n",
    "    \"\"\"\n",
    "    def sparse_cosine_sim(X,Y):\n",
    "        def my_dot(X,Y):\n",
    "            dot_prod_arr = X.dot(Y.T).data\n",
    "            return dot_prod_arr[0] if len(dot_prod_arr) > 0 else 0\n",
    "        def my_norm(Z):\n",
    "            return math.sqrt(my_dot(Z,Z))\n",
    "        doc_norm = my_norm(Y)\n",
    "        return my_dot(X,Y)/doc_norm if doc_norm !=0 else 0\n",
    "    def dense_cosine_sim(X,Y):\n",
    "        doc_norm = np.linalg.norm(Y)\n",
    "        return np.dot(X,Y)/doc_norm if doc_norm !=0 else 0\n",
    "    def is_array(X):\n",
    "        return isinstance(X, list) or isinstance(X, (np.ndarray, np.generic) )\n",
    "    return dense_cosine_sim(X,Y) if is_array(X) and is_array(Y) else sparse_cosine_sim(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search(query, doc_vectors, vectorizer, k=10, idx_map=None):\n",
    "    \"\"\" Returns top k simialr docs to query\n",
    "    =Inputs=\n",
    "    query: string\n",
    "        Input query to match with docs\n",
    "    docs_vecotrs: vector list\n",
    "        doc vectors to find similarity with query\n",
    "    vectorizer: tfidfvectorizer\n",
    "        the vectorizer used on the doc vectors\n",
    "    k: int\n",
    "        how many top docs to return\n",
    "    idx_map: dict or None\n",
    "        the map from idx to vid_id\n",
    "    =Outputs=\n",
    "    (score, vid_id) list\n",
    "    \"\"\"\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    sorted_k_tuples = sorted([(cosine_similarity(query_vector, doc_vector), idx) for idx,doc_vector in enumerate(doc_vectors)], reverse=True)[:k]\n",
    "    if idx_map is None:\n",
    "        return sorted_k_tuples\n",
    "    else:\n",
    "        return [(sim, idx_map[doc_idx]) for sim, doc_idx in sorted_k_tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.20982795772914034, 'ZyU213nhrh0'),\n",
       " (0.19621449999957852, '7eJpWOY3r18'),\n",
       " (0.16761168677345412, 'lce5gWKgMXI'),\n",
       " (0.16701550267293958, 'geyAFbSDPVk'),\n",
       " (0.15621381645353544, 'hX1YVzdnpEc'),\n",
       " (0.15454440509127793, 'ZJfUB_GRzEk'),\n",
       " (0.13923551930924907, '95KTrtzOY-g'),\n",
       " (0.13569190060186481, 'ej_H8wYo2s4'),\n",
       " (0.13129561210298837, 'ehYoIKTsiV0'),\n",
       " (0.12794194766063388, 'AijEQN6AuRs')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(\"thanks obama\", comments_vectors, tfv, k=10, idx_map=flat_comments_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
