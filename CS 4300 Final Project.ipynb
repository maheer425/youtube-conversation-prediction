{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube Conversation Prediction\n",
    "## CS/INFO 4300 Language and Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the data from the JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data_wo_replies.json') as json_file:   \n",
    "    video_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "video_num_comments, video_captions = np.array([ (video_datum[\"num_comments\"], video_datum[\"transcript\"]) \n",
    "                                              for video_datum in video_data ]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-83-62a5dbd59bb5>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-83-62a5dbd59bb5>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    \"\"\"\"\u001b[0m\n\u001b[1;37m        \n^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "with open('data_wo_replies.json') as json_file:   \n",
    "    video_data = json.load(json_file)\n",
    "video_num_comments, video_captions = np.array([ (video_data[video_id][\"num_comments\"], video_data[video_id][\"captions\"]) \n",
    "                                  for video_id in videos_data.keys() ]).T\n",
    "\"\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Consolidate caption text for each video into one string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_video_captions = []\n",
    "video_num_comments_cut  = []\n",
    "for caption_data_list,num_comments in zip(video_captions,video_num_comments):\n",
    "    text = \"\"\n",
    "    if caption_data_list is not None:\n",
    "        video_num_comments_cut.append(num_comments)\n",
    "        for caption_data in caption_data_list:\n",
    "            if caption_data is not None and \"text\" in caption_data:\n",
    "                text += (caption_data[\"text\"]+\" \")\n",
    "        combined_video_captions.append(text[:-1])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "video_captions = combined_video_captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2. Make a 50-50 train-test split.\n",
    "\n",
    "Use `sklearn.cross_validation.train_test_split`. Set `random_state=0`. Make sure the train and test sizes are equal (plus/minus one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1264\n",
      "1264\n"
     ]
    }
   ],
   "source": [
    "print(len(video_num_comments_cut))\n",
    "print(len(combined_video_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_comments_train, num_comments_test, video_captions_train, video_captions_test  = train_test_split(video_num_comments_cut, combined_video_captions, \n",
    "                                                                       test_size=.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "632\n",
      "632\n",
      "[16567, 1213, 109, 806, 13886, 25447, 517, 13534, 2112, 32, 2525, 963, 557, 4663, 809, 3854, 236, 5831, 280, 16, 45914, 5690, 5186, 405, 960, 6377, 5969, 177, 4063, 1691, 2067, 329, 119, 29434, 1860, 34, 25921, 77, 252, 451, 381, 587, 1301, 2347, 2475, 222, 3253, 385, 981, 44, 1438, 1304, 27, 627, 722, 15, 29, 145, 10807, 737, 966, 705, 385, 11, 1070, 875, 1089, 1175, 754, 1903, 363, 3734, 223, 1136, 181, 2898, 4791, 26, 464, 11837, 4099, 8710, 8225, 503, 1706, 2451, 23, 1382, 8548, 8757, 13657, 24, 139, 8858, 248, 1136, 120, 373, 16013, 21252, 1369, 3, 903, 995, 1249, 1552, 89, 13949, 139081, 30043, 1561, 1819, 9191, 17, 3766, 636, 3218, 182, 46, 232, 184, 141, 153, 9842, 3291, 971, 1551, 154, 22, 3670, 3544, 6487, 7745, 904, 11479, 1800, 1333, 91, 3764, 358, 256, 128, 478, 1086, 66, 17893, 662, 1400, 492, 1804, 487, 12442, 21, 229, 9670, 750, 20499, 2473, 17127, 18228, 3069, 2549, 875, 840, 141, 1772, 1130, 4272, 27, 652, 681, 630, 3131, 19775, 160, 2762, 2219, 51, 1697, 36, 136, 12590, 577, 353, 141221, 9, 26995, 6352, 1716, 24, 15151, 2172, 1811, 3925, 3457, 241, 1916, 411, 6776, 670, 1789, 1311, 1121, 578, 1009, 4035, 316, 17502, 6539, 2019, 1211, 142, 948, 1679, 690, 8459, 448, 4523, 101, 12523, 8976, 6841, 1584, 45, 475, 19, 333, 7857, 5160, 169, 3675, 339, 1318, 316, 6175, 959, 73, 4770, 431, 155, 2473, 778, 4874, 1125, 1268, 7378, 11699, 1197, 241, 23700, 7549, 7743, 1318, 1834, 178, 5425, 853, 3593, 3960, 74, 5269, 36609, 182, 3594, 108, 15833, 1572, 38470, 111, 2432, 3031, 1581, 14428, 4485, 61, 1988, 1355, 2136, 311, 1101, 336, 2598, 2356, 4834, 851, 1243, 138, 8057, 1516, 5837, 46, 4293, 1407, 5, 3743, 6629, 2177, 996, 472, 602, 507, 6473, 4832, 24, 24277, 286, 4822, 179, 12947, 841, 21117, 93, 17521, 103, 2063, 1565, 14010, 2350, 2355, 2666, 4115, 160, 1218, 8956, 1584, 271, 2378, 365, 3015, 137716, 5421, 4543, 530, 11925, 2738, 1437, 41, 1038, 6003, 10479, 8869, 7150, 619, 1667, 3746, 3066, 478, 3, 8254, 9131, 73593, 55, 73, 149, 20, 3873, 1953, 148, 1290, 2081, 408, 1593, 1530, 8, 181, 17176, 778, 1106, 1932, 193, 439, 666, 520, 584, 2, 1829, 1578, 940, 957, 1743, 12831, 237, 13848, 38, 2864, 14, 32, 193, 1517, 929, 1659, 6071, 129, 525, 2893, 1223, 59, 285, 589, 3451, 4610, 2753, 9390, 36, 103, 445, 984, 4198, 5274, 358, 1555, 565, 14240, 11551, 394, 1408, 4773, 81, 6011, 12655, 14845, 130381, 2336, 5671, 62, 512, 41, 1305, 1886, 329, 568, 589, 3126, 1916, 786, 154, 1791, 10340, 4010, 577, 84, 316, 426, 541, 60, 492, 194, 10324, 799, 3179, 3798, 1, 8157, 19, 313, 2610, 2226, 388, 1944, 4338, 246, 1249, 3220, 1050, 722, 1576, 670, 933, 1147, 453, 1219, 1066, 39, 24755, 13945, 100, 134, 190, 806, 7723, 14915, 1534, 1215, 10594, 4201, 977, 18, 157, 124, 2474, 1042, 33701, 5888, 9791, 11401, 49, 10719, 201, 481, 4726, 2875, 374, 21, 7402, 3406, 679, 1009, 18753, 11448, 336, 11227, 12437, 1970, 3492, 511, 6399, 625, 219, 1378, 17590, 980, 3687, 32, 2281, 117006, 1550, 72, 457, 322, 1261, 345, 252, 575, 324, 1021, 152, 6471, 2810, 38, 334, 144682, 160, 1175, 1049, 155671, 251, 395, 2857, 12, 1891, 6388, 391, 900, 1030, 39, 186, 1349, 1958, 10275, 159, 7544, 579, 53, 702, 2541, 8552, 1392, 1109, 18, 2075, 207, 6428, 11313, 2782, 150985, 1401, 10313, 862, 4882, 108, 382, 1643, 443, 745, 87613, 635, 2632, 11589, 723, 14677, 1285, 1272, 5712, 243, 11239, 258, 4373, 5282, 459, 586, 315, 6759, 3685, 1342, 533, 269, 1158, 1166, 5561, 2926, 132206, 34169, 597, 67, 183, 201, 1554, 181, 5134, 2085, 41, 13914, 82303, 2557, 566, 1300, 13144, 17427, 715, 242, 101, 12202]\n",
      "632\n",
      "632\n"
     ]
    }
   ],
   "source": [
    "print(len(num_comments_test))\n",
    "print(len(num_comments_train))\n",
    "print(num_comments_test)\n",
    "print(len(video_captions_test))\n",
    "print(len(video_captions_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build the document-term matrices\n",
    "\n",
    "Use `sklearn.feature_extraction.TfidfVectorizer`. Use unigrams only, disable idf, use `l1` normalization. \n",
    "\n",
    "Resulting matrices are `X_train` and `X_test`.\n",
    "\n",
    "**Note:** Remember to just `fit` on the training data. If a word occurs only in the test documents, our model should **not** be aware that the word exists, as we are trying to evaluate the performance on completely unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(ngram_range=(1,2), lowercase=True, strip_accents=\"unicode\", \n",
    "                      stop_words='english', use_idf=False, norm='l1', min_df=1)\n",
    "tfv.fit(video_captions_train)\n",
    "X_train = tfv.transform(video_captions_train)\n",
    "X_test  = tfv.transform(video_captions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(632, 196144)\n",
      "(632, 196144)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Predict using a random guess baseline\n",
    "\n",
    "Use a random classifier from `sklearn.dummy.DummyClassifier`.  Set `strategy=\"stratified\"`? Set `random_state=0`, to get the same result every time, since randomness is involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dummy = DummyClassifier(strategy=\"stratified\", random_state=0)\n",
    "dummy.fit(X_train, num_comments_train)\n",
    "num_comments_pred_stratified = dummy.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluate the randomized predictions\n",
    "\n",
    "We will use a regression evaluation statistic called mean absolute error initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_mae_score = mean_absolute_error(num_comments_test, num_comments_pred_stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8253.64398734\n"
     ]
    }
   ],
   "source": [
    "print(my_mae_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train and evaluate a classifier.\n",
    "\n",
    "We will use `sklearn.svm.SVR()\" as our initial classifier (Support Vector Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm_regression_classifier = SVR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.0,\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_regression_classifier.fit(X_test, num_comments_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_comments_pred_svr = svm_regression_classifier.predict(X_test)\n",
    "my_mae_score_svr = mean_absolute_error(num_comments_test, num_comments_pred_svr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5450.37658179\n"
     ]
    }
   ],
   "source": [
    "print(my_mae_score_svr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Use grid search and cross-validation to tune the classifier\n",
    "\n",
    "The score above is pretty disappointing, but kind of expected, given how little work we did-- we are basically just using the default configuration.  A `LinearSVC` has a bunch of configuration options that should be tweaked:\n",
    "\n",
    "* `C` is the *regularization parameter*. Lower values of C constraint the model more, while higher values allows the model to fit the training data better. (Remember that fitting the training data too well can lead to overfitting.)\n",
    "\n",
    "* `class_weight` can force the classifier to emphasize positive instances more or less than negative ones. This is useful if we know for a fact that the classes aren't equally probable. Read the documentation and see what the `'auto'` setting does.\n",
    "\n",
    "However, choosing these values should also be done without looking at the test data, because they are part of the model. Use `sklearn.grid_search.GridSearchCV` to systematically try out different values for these two parameters, and choose the configuration that does best.\n",
    "\n",
    "`GridSearchCV` uses k-fold cross-validation to ensure fair evaluation and avoid overfitting. This consists of splitting the training data into *k* parts, then training the classifier *k* times, each time leaving out a different part, that is used for scoring. The average score over the *k* folds is a better estimate of how well the classifier would generalize.\n",
    "\n",
    "Because we are facing a multi-label problem, the default scoring strategy (accuracy) doesn't make sense. We have to define our own `sample_f1_scorer` strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_f1_scorer(estimator, X, y):\n",
    "    \"\"\"sample-f1 scorer metric\n",
    "    \n",
    "    This function is just glue code for the scikit-learn scorer API.\n",
    "    See http://scikit-learn.org/stable/modules/model_evaluation.html#implementing-your-own-scoring-object\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    \n",
    "    estimator:\n",
    "        the model that should be evaluated (e.g., the scikit-learn classifier)\n",
    "    X: array-like, shape (n_samples, n_features)\n",
    "        the test data\n",
    "    y: array-like, shape (n_samples, n_labels)\n",
    "        the ground truth target for X.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    \n",
    "    sample_f1_score, float\n",
    "        the sample F1 score as used in Q06 and Q07\n",
    "    \"\"\"\n",
    "    y_pred = estimator.predict(X)\n",
    "    return f1_score(y, y_pred, average='samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run grid search over a range of regularization parameters, as below.  This takes under 1 minute on a 2014 MacBook Pro Retina. If you're not sure your code works, test it on a small number of documents first to avoid wasting time.\n",
    "\n",
    "What is the best configuration, and the best score (averaged over the 3 folds)? (there are attributes of the `GridSearchCV` object that answer this).\n",
    "\n",
    "DISCUSSION ITEM.\n",
    "What can you say about the impact of `C` and `class_weight` on the score? (look at `grid.grid_scores_` to answer this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = dict(\n",
    "    estimator__C=[1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3],  # you can also build this using np.logspace\n",
    "    estimator__class_weight=['auto', None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ovr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-109-9233726c6da1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m grid = GridSearchCV(ovr,\n\u001b[0m\u001b[0;32m      2\u001b[0m                     \u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                     \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                     \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_f1_scorer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                     verbose=True)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ovr' is not defined"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(ovr,\n",
    "                    param_grid,\n",
    "                    cv=3,\n",
    "                    scoring=sample_f1_scorer,\n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done  42 out of  42 | elapsed:   28.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 14 candidates, totalling 42 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "       estimator=OneVsRestClassifier(estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',\n",
       "     random_state=None, tol=0.0001, verbose=0),\n",
       "          n_jobs=1),\n",
       "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
       "       param_grid={'estimator__class_weight': ['auto', None], 'estimator__C': [0.001, 0.01, 0.1, 1, 10.0, 100.0, 1000.0]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, score_func=None,\n",
       "       scoring=<function sample_f1_scorer at 0x7feeaf225500>, verbose=True)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Evaluate the chosen classifier on the test set. Inspect performance on individual categories.\n",
    "\n",
    "Use `grid.best_estimator_` to access the `ovr` object chosen as best by the grid search. Use `sample_f1_scorer` and report the **sample F1** score as in Q06 and Q07. This time, you should see a rewarding increase.\n",
    "\n",
    "DISCUSSION ITEM.\n",
    "Compare this score with the cross-validated average score over the 3 folds for the best model (Q08).  Does cross-validation give a reasonable estimate of the actual generalization performance a model can get on unseen test data? Compare with what we saw in class, when we were looking at the performance of a classifier on the data it was trained on, versus on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_f1_score = sample_f1_scorer(grid.best_estimator_, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.563988708163\n"
     ]
    }
   ],
   "source": [
    "print(grid_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** TODO discuss **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Then, to aggregate scores over individual categories, use `sklearn.metrics.classification_report`. Keep in mind that in the classification report, precision, recall and F1 have different meaning than the sample-based scores we used in the previous questions: they are averages over a given label, as opposed to a given document.\n",
    "\n",
    "DISCUSSION ITEM. How do you interpret this table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_pred_test_grid = grid.predict(X_test)\n",
    "grid_report = classification_report(Y_test, Y_pred_test_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.73      0.65        78\n",
      "          1       0.49      0.57      0.53        54\n",
      "          2       0.61      0.66      0.64        89\n",
      "          3       0.49      0.73      0.59        52\n",
      "          4       0.69      0.65      0.67       156\n",
      "          5       0.39      0.41      0.40        39\n",
      "          6       0.64      0.49      0.56        55\n",
      "          7       0.29      0.39      0.33        46\n",
      "          8       0.36      0.46      0.41        68\n",
      "          9       0.68      0.67      0.68        61\n",
      "         10       0.66      0.72      0.69       120\n",
      "\n",
      "avg / total       0.57      0.62      0.59       818\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(grid_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
