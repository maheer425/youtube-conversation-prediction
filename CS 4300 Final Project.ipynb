{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube Conversation Prediction\n",
    "## CS/INFO 4300 Language and Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import httplib2\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from apiclient.discovery import build\n",
    "from apiclient.errors import HttpError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the data from the JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/data_wo_replies.json') as json_file:   \n",
    "    video_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "video_num_comments, video_captions = np.array([ (video_datum[\"num_comments\"], video_datum[\"transcript\"]) \n",
    "                                              for video_datum in video_data ]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith open(\\'data_wo_replies.json\\') as json_file:   \\n    video_data = json.load(json_file)\\nvideo_num_comments, video_captions = np.array([ (video_data[video_id][\"num_comments\"], video_data[video_id][\"captions\"]) \\n                                  for video_id in videos_data.keys() ]).T\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "with open('data_wo_replies.json') as json_file:   \n",
    "    video_data = json.load(json_file)\n",
    "video_num_comments, video_captions = np.array([ (video_data[video_id][\"num_comments\"], video_data[video_id][\"captions\"]) \n",
    "                                  for video_id in videos_data.keys() ]).T\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Consolidate caption text for each video into one string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_video_captions = []\n",
    "video_num_comments_cut  = []\n",
    "for caption_data_list,num_comments in zip(video_captions,video_num_comments):\n",
    "    text = \"\"\n",
    "    if caption_data_list is not None:\n",
    "        video_num_comments_cut.append(num_comments)\n",
    "        for caption_data in caption_data_list:\n",
    "            if caption_data is not None and \"text\" in caption_data:\n",
    "                text += (caption_data[\"text\"]+\" \")\n",
    "        combined_video_captions.append(text[:-1])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "video_captions = combined_video_captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2. Make a 75-25 train-test split.\n",
    "\n",
    "Use `sklearn.cross_validation.train_test_split`. Set `random_state=0`. Make sure the train and test sizes are equal (plus/minus one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1264\n",
      "1264\n"
     ]
    }
   ],
   "source": [
    "print(len(video_num_comments_cut))\n",
    "print(len(combined_video_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_comments_train, num_comments_test, video_captions_train, video_captions_test  = train_test_split(video_num_comments_cut, combined_video_captions, \n",
    "                                                                       test_size=.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316\n",
      "948\n",
      "[16567, 1213, 109, 806, 13886, 25447, 517, 13534, 2112, 32, 2525, 963, 557, 4663, 809, 3854, 236, 5831, 280, 16, 45914, 5690, 5186, 405, 960, 6377, 5969, 177, 4063, 1691, 2067, 329, 119, 29434, 1860, 34, 25921, 77, 252, 451, 381, 587, 1301, 2347, 2475, 222, 3253, 385, 981, 44, 1438, 1304, 27, 627, 722, 15, 29, 145, 10807, 737, 966, 705, 385, 11, 1070, 875, 1089, 1175, 754, 1903, 363, 3734, 223, 1136, 181, 2898, 4791, 26, 464, 11837, 4099, 8710, 8225, 503, 1706, 2451, 23, 1382, 8548, 8757, 13657, 24, 139, 8858, 248, 1136, 120, 373, 16013, 21252, 1369, 3, 903, 995, 1249, 1552, 89, 13949, 139081, 30043, 1561, 1819, 9191, 17, 3766, 636, 3218, 182, 46, 232, 184, 141, 153, 9842, 3291, 971, 1551, 154, 22, 3670, 3544, 6487, 7745, 904, 11479, 1800, 1333, 91, 3764, 358, 256, 128, 478, 1086, 66, 17893, 662, 1400, 492, 1804, 487, 12442, 21, 229, 9670, 750, 20499, 2473, 17127, 18228, 3069, 2549, 875, 840, 141, 1772, 1130, 4272, 27, 652, 681, 630, 3131, 19775, 160, 2762, 2219, 51, 1697, 36, 136, 12590, 577, 353, 141221, 9, 26995, 6352, 1716, 24, 15151, 2172, 1811, 3925, 3457, 241, 1916, 411, 6776, 670, 1789, 1311, 1121, 578, 1009, 4035, 316, 17502, 6539, 2019, 1211, 142, 948, 1679, 690, 8459, 448, 4523, 101, 12523, 8976, 6841, 1584, 45, 475, 19, 333, 7857, 5160, 169, 3675, 339, 1318, 316, 6175, 959, 73, 4770, 431, 155, 2473, 778, 4874, 1125, 1268, 7378, 11699, 1197, 241, 23700, 7549, 7743, 1318, 1834, 178, 5425, 853, 3593, 3960, 74, 5269, 36609, 182, 3594, 108, 15833, 1572, 38470, 111, 2432, 3031, 1581, 14428, 4485, 61, 1988, 1355, 2136, 311, 1101, 336, 2598, 2356, 4834, 851, 1243, 138, 8057, 1516, 5837, 46, 4293, 1407, 5, 3743, 6629, 2177, 996, 472, 602, 507, 6473, 4832, 24, 24277, 286, 4822, 179, 12947, 841, 21117, 93, 17521, 103, 2063, 1565]\n",
      "316\n",
      "948\n"
     ]
    }
   ],
   "source": [
    "print(len(num_comments_test))\n",
    "print(len(num_comments_train))\n",
    "print(num_comments_test)\n",
    "print(len(video_captions_test))\n",
    "print(len(video_captions_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build the document-term matrices\n",
    "\n",
    "Use `sklearn.feature_extraction.TfidfVectorizer`. Use unigrams only, disable idf, use `l1` normalization. \n",
    "\n",
    "Resulting matrices are `X_train` and `X_test`.\n",
    "\n",
    "**Note:** Remember to just `fit` on the training data. If a word occurs only in the test documents, our model should **not** be aware that the word exists, as we are trying to evaluate the performance on completely unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(ngram_range=(1,2), lowercase=True, strip_accents=\"unicode\", \n",
    "                      stop_words='english', use_idf=False, norm='l1', min_df=1)\n",
    "tfv.fit(video_captions_train)\n",
    "X_train = tfv.transform(video_captions_train)\n",
    "X_test  = tfv.transform(video_captions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(948, 277057)\n",
      "(316, 277057)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Predict using a random guess baseline\n",
    "\n",
    "Use a random classifier from `sklearn.dummy.DummyClassifier`.  Set `strategy=\"stratified\"`? Set `random_state=0`, to get the same result every time, since randomness is involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dummy = DummyClassifier(strategy=\"stratified\", random_state=0)\n",
    "dummy.fit(X_train, num_comments_train)\n",
    "num_comments_pred_stratified = dummy.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluate the randomized predictions\n",
    "\n",
    "We will use a regression evaluation statistic called mean absolute error initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_mae_score = mean_absolute_error(num_comments_test, num_comments_pred_stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7057.46518987\n"
     ]
    }
   ],
   "source": [
    "print(my_mae_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train and evaluate SVM Regression.\n",
    "\n",
    "We will use `sklearn.svm.SVR()\" as our initial classifier (Support Vector Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm_regression_classifier = SVR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.0,\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_regression_classifier.fit(X_train, num_comments_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_comments_pred_svr = svm_regression_classifier.predict(X_test)\n",
    "my_mae_score_svr = mean_absolute_error(num_comments_test, num_comments_pred_svr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4390.5664555\n"
     ]
    }
   ],
   "source": [
    "print(my_mae_score_svr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Train and evaluate Linear Regression.\n",
    "\n",
    "We will be using `sklearn.linear_model.LinearRegression()\" as an additional classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_regression_classifier = LinearRegression(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_regression_classifier.fit(X_train, num_comments_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_comments_pred_linreg = linear_regression_classifier.predict(X_test)\n",
    "mae_score_linreg = mean_absolute_error(num_comments_test, num_comments_pred_linreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12237.1164087\n"
     ]
    }
   ],
   "source": [
    "print(mae_score_linreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Train and evaluate Lasso Regression.\n",
    "\n",
    "We will be using \"sklearn.linear_model.Lasso\" as an additional classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lasso_classifier = Lasso()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=None,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_classifier.fit(X_train, num_comments_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_comments_pred_lasso = lasso_classifier.predict(X_test)\n",
    "mae_score_lasso = mean_absolute_error(num_comments_test, num_comments_pred_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004.37201691\n"
     ]
    }
   ],
   "source": [
    "print(mae_score_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Create a query search\n",
    "\n",
    "Return the top ten search results for a given query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Key and version data \n",
    "DEVELOPER_KEY = \"AIzaSyBEuuLWPO0AJIIp7TVGIB1uM_mNiNkMVbw\"\n",
    "YOUTUBE_READ_WRITE_SCOPE = \"https://www.googleapis.com/auth/youtube\"\n",
    "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
    "YOUTUBE_API_VERSION = \"v3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Authenticate \n",
    "youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=DEVELOPER_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Returns the top ten search results for the query in the form \n",
    "   {video_id:{\"thumbnail\":thumbnail_url, \"title\":video title}}\"\"\"\n",
    "\n",
    "def query_search(query):\n",
    "    search_request = youtube.search().list(part=\"snippet\", q=query, maxResults=10)\n",
    "    \n",
    "    search_response = search_request.execute()\n",
    "    \n",
    "    search_results = {}\n",
    "    for search_result in search_response[\"items\"]:\n",
    "        video_id = search_result[\"id\"][\"videoId\"]\n",
    "        thumbnail = search_result[\"snippet\"][\"thumbnails\"][\"default\"][\"url\"]\n",
    "        title = search_result[\"snippet\"][\"title\"]\n",
    "        search_results[video_id] = {\"thumbnail\":thumbnail, \"title\":title}\n",
    "    \n",
    "    return search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'0E8lsv-Vcm4': {'thumbnail': u'https://i.ytimg.com/vi/0E8lsv-Vcm4/default.jpg',\n",
       "  'title': u'Tomato Soup By Sanjeev Kapoor'},\n",
       " u'2KR44a_5v_A': {'thumbnail': u'https://i.ytimg.com/vi/2KR44a_5v_A/default.jpg',\n",
       "  'title': u\"Gordon Ramsay's Broccoli Soup Recipe\"},\n",
       " u'5Ceg1QN56p0': {'thumbnail': u'https://i.ytimg.com/vi/5Ceg1QN56p0/default.jpg',\n",
       "  'title': u'Korean Chicken Noodle Soup from Scratch (Kalguksu: \\uce7c\\uad6d\\uc218)'},\n",
       " u'GojmNjoTaTg': {'thumbnail': u'https://i.ytimg.com/vi/GojmNjoTaTg/default.jpg',\n",
       "  'title': u'Loaded Potato Soup Recipe - Laura Vitale - Laura in the Kitchen Episode 863'},\n",
       " u'L1TFnkm1TG8': {'thumbnail': u'https://i.ytimg.com/vi/L1TFnkm1TG8/default.jpg',\n",
       "  'title': u'Fall Soup - 3 Delicious Ways'},\n",
       " u'NY0aQJmmTT4': {'thumbnail': u'https://i.ytimg.com/vi/NY0aQJmmTT4/default.jpg',\n",
       "  'title': u'Vegetarian Sweet Corn Soup'},\n",
       " u'Q6xc2IM6KVE': {'thumbnail': u'https://i.ytimg.com/vi/Q6xc2IM6KVE/default.jpg',\n",
       "  'title': u'Hungarian Mushroom Soup Recipe'},\n",
       " u'_mrkQmEHquk': {'thumbnail': u'https://i.ytimg.com/vi/_mrkQmEHquk/default.jpg',\n",
       "  'title': u'How to Make Excellent Broccoli Cheese Soup'},\n",
       " u'kKNqyE3OvZU': {'thumbnail': u'https://i.ytimg.com/vi/kKNqyE3OvZU/default.jpg',\n",
       "  'title': u'How to Make the Best Homemade Chicken Soup from Scratch ... Home Made Chicken Soup Recipe'},\n",
       " u'mZyR2Ew66w8': {'thumbnail': u'https://i.ytimg.com/vi/mZyR2Ew66w8/default.jpg',\n",
       "  'title': u'Magic Diet Soup -  Lose Weight Fast - Low Gi.'}}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_search(\"soup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Use grid search and cross-validation to tune the classifier\n",
    "\n",
    "The score above is pretty disappointing, but kind of expected, given how little work we did-- we are basically just using the default configuration.  A `LinearSVC` has a bunch of configuration options that should be tweaked:\n",
    "\n",
    "* `C` is the *regularization parameter*. Lower values of C constraint the model more, while higher values allows the model to fit the training data better. (Remember that fitting the training data too well can lead to overfitting.)\n",
    "\n",
    "* `class_weight` can force the classifier to emphasize positive instances more or less than negative ones. This is useful if we know for a fact that the classes aren't equally probable. Read the documentation and see what the `'auto'` setting does.\n",
    "\n",
    "However, choosing these values should also be done without looking at the test data, because they are part of the model. Use `sklearn.grid_search.GridSearchCV` to systematically try out different values for these two parameters, and choose the configuration that does best.\n",
    "\n",
    "`GridSearchCV` uses k-fold cross-validation to ensure fair evaluation and avoid overfitting. This consists of splitting the training data into *k* parts, then training the classifier *k* times, each time leaving out a different part, that is used for scoring. The average score over the *k* folds is a better estimate of how well the classifier would generalize.\n",
    "\n",
    "Because we are facing a multi-label problem, the default scoring strategy (accuracy) doesn't make sense. We have to define our own `sample_f1_scorer` strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_f1_scorer(estimator, X, y):\n",
    "    \"\"\"sample-f1 scorer metric\n",
    "    \n",
    "    This function is just glue code for the scikit-learn scorer API.\n",
    "    See http://scikit-learn.org/stable/modules/model_evaluation.html#implementing-your-own-scoring-object\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    \n",
    "    estimator:\n",
    "        the model that should be evaluated (e.g., the scikit-learn classifier)\n",
    "    X: array-like, shape (n_samples, n_features)\n",
    "        the test data\n",
    "    y: array-like, shape (n_samples, n_labels)\n",
    "        the ground truth target for X.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    \n",
    "    sample_f1_score, float\n",
    "        the sample F1 score as used in Q06 and Q07\n",
    "    \"\"\"\n",
    "    y_pred = estimator.predict(X)\n",
    "    return f1_score(y, y_pred, average='samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run grid search over a range of regularization parameters, as below.  This takes under 1 minute on a 2014 MacBook Pro Retina. If you're not sure your code works, test it on a small number of documents first to avoid wasting time.\n",
    "\n",
    "What is the best configuration, and the best score (averaged over the 3 folds)? (there are attributes of the `GridSearchCV` object that answer this).\n",
    "\n",
    "DISCUSSION ITEM.\n",
    "What can you say about the impact of `C` and `class_weight` on the score? (look at `grid.grid_scores_` to answer this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = dict(\n",
    "    estimator__C=[1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3],  # you can also build this using np.logspace\n",
    "    estimator__class_weight=['auto', None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ovr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-9233726c6da1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m grid = GridSearchCV(ovr,\n\u001b[0m\u001b[0;32m      2\u001b[0m                     \u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                     \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                     \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_f1_scorer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                     verbose=True)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ovr' is not defined"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(ovr,\n",
    "                    param_grid,\n",
    "                    cv=3,\n",
    "                    scoring=sample_f1_scorer,\n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Evaluate the chosen classifier on the test set. Inspect performance on individual categories.\n",
    "\n",
    "Use `grid.best_estimator_` to access the `ovr` object chosen as best by the grid search. Use `sample_f1_scorer` and report the **sample F1** score as in Q06 and Q07. This time, you should see a rewarding increase.\n",
    "\n",
    "DISCUSSION ITEM.\n",
    "Compare this score with the cross-validated average score over the 3 folds for the best model (Q08).  Does cross-validation give a reasonable estimate of the actual generalization performance a model can get on unseen test data? Compare with what we saw in class, when we were looking at the performance of a classifier on the data it was trained on, versus on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_f1_score = sample_f1_scorer(grid.best_estimator_, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(grid_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** TODO discuss **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Then, to aggregate scores over individual categories, use `sklearn.metrics.classification_report`. Keep in mind that in the classification report, precision, recall and F1 have different meaning than the sample-based scores we used in the previous questions: they are averages over a given label, as opposed to a given document.\n",
    "\n",
    "DISCUSSION ITEM. How do you interpret this table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_pred_test_grid = grid.predict(X_test)\n",
    "grid_report = classification_report(Y_test, Y_pred_test_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(grid_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
