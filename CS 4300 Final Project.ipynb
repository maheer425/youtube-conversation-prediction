{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube Conversation Prediction\n",
    "## CS/INFO 4300 Language and Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import httplib2\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from apiclient.discovery import build\n",
    "from apiclient.errors import HttpError\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the data from the JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/big_data_approx.json') as json_file:   \n",
    "    video_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "video_num_comments, video_captions = np.array([ (video_datum[\"score\"], video_datum[\"captions\"]) \n",
    "                                              for _,video_datum in video_data.iteritems() ]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith open(\\'data_wo_replies.json\\') as json_file:   \\n    video_data = json.load(json_file)\\nvideo_num_comments, video_captions = np.array([ (video_data[video_id][\"num_comments\"], video_data[video_id][\"captions\"]) \\n                                  for video_id in videos_data.keys() ]).T\\n'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "with open('data_wo_replies.json') as json_file:   \n",
    "    video_data = json.load(json_file)\n",
    "video_num_comments, video_captions = np.array([ (video_data[video_id][\"num_comments\"], video_data[video_id][\"captions\"]) \n",
    "                                  for video_id in videos_data.keys() ]).T\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a stemmer for use with our captions\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Consolidate caption text for each video into one string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_video_captions = []\n",
    "video_num_comments_cut  = []\n",
    "for caption_data_list,num_comments in zip(video_captions,video_num_comments):\n",
    "    text = \"\"\n",
    "    if caption_data_list is not None:\n",
    "        video_num_comments_cut.append(num_comments)\n",
    "        for caption_data in caption_data_list:\n",
    "            if caption_data is not None and \"text\" in caption_data:\n",
    "                for word in caption_data[\"text\"].split():\n",
    "                    text += (stemmer.stem(word)+\" \")\n",
    "        combined_video_captions.append(text[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "video_captions = combined_video_captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2. Make a 75-25 train-test split.\n",
    "\n",
    "Use `sklearn.cross_validation.train_test_split`. Set `random_state=0`. Make sure the train and test sizes are equal (plus/minus one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2268\n",
      "2268\n"
     ]
    }
   ],
   "source": [
    "print(len(video_num_comments_cut))\n",
    "print(len(combined_video_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_comments_train, num_comments_test, video_captions_train, video_captions_test  = train_test_split(video_num_comments_cut, combined_video_captions, \n",
    "                                                                       test_size=.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567\n",
      "1701\n",
      "[3.178205413428691, 5.475734948114026, 11.58947304983937, 7.303907401565681, 86.34807407802293, 0.0, 15.717823106187474, 8.312635394009508, 24.841909211275887, 7.74745102458439, 0.0, 5.921564646567184, 4.286799108345786, 3.763889246568423, 74.25283088917766, 77.14749359321102, 6.298189436278897, 24.114930733709596, 23.154533407978025, 27.936078002388232, 12.361523666864208, 0.0, 39.97149858361863, 49.23253655259419, 90.69994002136518, 7.551373412374421, 5.422468385072719, 7.850496637370607, 13.37173421106768, 20.58451060326041, 17.02175948253851, 137.0263455987138, 11.261385879524001, 19.513881338875276, 6.960199505973925, 5.822296013041943, 50.77502432969916, 21.84879495930093, 3.2528869371567266, 5.022133583323983, 13.1252943370363, 35.92047575639189, 10.305893293260185, 2.710596398972684, 8.007115721151713, 23.122630602532734, 14.706157601880248, 8.679733611084808, 22.641049383611943, 6.038108339415196, 18.729388308166865, 32.077738458143656, 21.22504762693614, 34.629094223021745, 118.3858348774964, 508.9886809203225, 28.809526350046415, 10.391059070950506, 24.241924086672615, 19.26000717872995, 78.19212923182678, 34.88031401584031, 145.7356915603298, 13.23058689326919, 38.32265516627175, 2.718993966538153, 302.51331697944323, 16.41612810506322, 0.0, 36.06561729733557, 2.747789555957208, 15.82082710472779, 21.53598381992172, 407.0453338116691, 83.5155153029267, 4.425251750847981, 105.97088027071553, 29.05139844419691, 19.274470844388393, 49.65824209822973, 256.1948761024779, 11.505084430993218, 600.4497091964275, 14.852917034732373, 0.7302094971047194, 13.885898289159456, 38.61099016652016, 123.56921737586521, 42.70057638507874, 30.514283031711383, 8.522380699556871, 23.752263887651793, 10.648641140793554, 1.6366595369685588, 9.593571178654868, 27.645593875260076, 2.9921528152285797, 25.916548713143676, 31.59511939819739, 10.523741871183208, 15.28867298817676, 6.277201205222631, 19.920610508122042, 0.0, 2.7010358472474194, 126.50961850741308, 600.8336068916808, 10.173109690655687, 8.936752167613752, 334.8979346216438, 35.86169123735875, 118.64110561411057, 22.571836927159676, 18.0802401055886, 15.244984268753301, 16.344653593617764, 92.12274742116398, 81.52575664772847, 0.0, 3.2320361755112117, 4.363329516368557, 4.523822449016521, 440.55171746648506, 3.9231310421666845, 5.918964415150493, 144.2698516209142, 8.59977509653917, 35.449796840031325, 26.892708848776923, 3.0802139037433154, 10.036894238822706, 18.167147782856226, 8.279155425027191, 57.44822179800966, 12.74596165448247, 20.394990990212804, 7.516039912449523, 17.31594121369805, 3.449194553602862, 6.889841380427919, 108.06715546864034, 73.59353183424317, 1.6276680480681542, 19.642961300792926, 3.9784581048954437, 67.04842774712914, 19.692550062154613, 35.42921614523082, 28.179383353854355, 14.449666393327144, 7.678722260615834, 7.319572536963841, 31.805893177635912, 12.16624179132349, 47.515761072742364, 6.559408975840744, 4.8552942330255, 8.097980676276963, 3.53835920922318, 11.53175316282355, 13.9526510036907, 4.723737483559637, 7.175307103144014, 12.820512820512821, 311.75979983319434, 44.32027287823706, 10.135575386961788, 0.5013188265988311, 2.233741319615534, 9.050256550956755, 2.8537093465322676, 21.855911573645216, 4.544811844789626, 19.177420634231236, 40.56725100770778, 40.541509828197555, 19.289062354853904, 28.70265564916885, 0.0, 7.949322187794788, 24.863918116363465, 9.78156226994023, 8.581918201750062, 16.190994863038902, 2.564474337009409, 23.07035210345494, 8.9889514666123, 10.045203415369162, 9.166338489115821, 5.806071408872258, 0.0, 50.78293232857594, 13.745824221884709, 8.146639511201629, 4.980834165201817, 29.208160990771567, 26.44802962179318, 9.357898270560462, 9.932420076907189, 53.175326648435124, 5.2068281294651575, 43.018968725054435, 37.53478885740074, 35.16684895354427, 6.651207819546478, 3.986922892911251, 10.529266169467798, 10.45073450840636, 81.44620124293985, 13.688019977832084, 219.19883756310503, 4.177807486631016, 0.12466900379492446, 13.582425652612136, 24.444766086103474, 11.472587820765709, 197.97138485321722, 33.24706123798557, 15.22051570403368, 38.36153616640382, 1.9154437976865695, 5.093285482569209, 19.90983300286647, 9.796078303319238, 72.30166167860777, 7.681430597981113, 0.0, 11.847294831779184, 3.8068828441822813, 237.85616111550266, 19.42541248649327, 3.7366956476646793, 1.3121478649335547, 11.375356427834738, 8.947618174895698, 157.2378858565934, 6.502047647403244, 9.42125039711666, 35.13086246267346, 3.791000165606849, 133.66190751331004, 42.558292872984474, 9.848063731203647, 1.0672472491702154, 65.66511550228121, 0.2459978456738665, 3.654142701580782, 156.6548648060605, 20.398385206901295, 13.75921375921376, 11.939146775958202, 54.09588495608466, 15.10108544153329, 9.750915930097271, 2.0609493228903832, 62.43436387387618, 2.889969982403478, 14.186707580430749, 31.396398196784418, 8.877980281139651, 26.499350669195646, 36.619018348743836, 28.624694344246276, 5.162484809535109, 17.225461942374057, 23.6295011871128, 20.309643956318645, 11.40131378215736, 4.904919207450359, 1.8405850466896818, 6.973421237941792, 1.539224572865181, 14.268936525184674, 2.2277531005991418, 66.42334424300371, 7.245210111093221, 5.98657809191792, 17.630132414887388, 56.41061801678216, 0.0, 3.2181522625572696, 21.16429695440824, 89.6120980902674, 7.402107133163907, 108.07643477977798, 181.35883501392522, 44.69631722520651, 8.90968236034547, 1.978166205524588, 4.390785205448831, 21.478634411033237, 41.060825351710726, 15.935553766439485, 162.51773507029537, 5.604946663566058, 7.403157446650996, 6.545502149106539, 26.320475007160717, 66.35024789755195, 27.300668426043078, 11.354750801929276, 3.3039592205241943, 27.245991233201643, 35.81960248571381, 44.392667161698924, 26.58949183282767, 39.229700039342426, 130.73573605510805, 2.2483831716014215, 185.85155279906903, 26.899807011796184, 55.946184246449654, 9.14968891057704, 37.17048001442656, 8.126014876211372, 4.2299232461200065, 0.0, 89.43229178343934, 31.94301420216534, 14.257782306092157, 45.15839260499326, 16.467027686645093, 5.364313521135395, 167.39549302581054, 15.316832684749292, 332.5571293269813, 30.597992771674175, 9.542135570178527, 13.374530526683552, 267.01186048786377, 64.31044363551123, 24.800928263315, 14.113454072105654, 17.19586734543289, 22.93889645993842, 214.15534892938308, 5.1148727346594, 10.104617636042125, 20.716190904458923, 100.19231830590073, 18.484965826716195, 25.22667623986688, 179.7093842528939, 13.03201485924052, 12.920232149707882, 0.0, 7.4726086019116815, 4.73458227668307, 29.820082754848443, 8.826165280128908, 22.392361822452393, 3.2721414185077484, 41.95885138850038, 2.8017114837729964, 6.368973645148861, 4.912644253099432, 149.30461668684825, 6.774146469997255, 30.031391303362383, 62.54732198702966, 31.69834700324548, 11.764705882352942, 0.0, 39.740882901771336, 15.266890307621004, 49.38509533732429, 8.221143335182338, 9.708081131999839, 3.1525851197982346, 9.1676586894353, 10.880837824512486, 6.195949024431802, 6.6597031875442525, 14.91081066187792, 30.619849040455094, 9.00455023717892, 6.910356281572422, 5.748500188560053, 4.824019000342663, 76.64391250302026, 10.833077176195937, 0.0, 21.092132505175982, 3.0118345667281665, 2.4486098017850364, 8.660825300868952, 19.54581455561062, 10.137640508248403, 9.317488716939927, 4.776026085786112, 11.991739807462038, 14.872300187892845, 7.585351481747952, 3.3689420296956927, 4.908604511007545, 39.799501482815714, 5.864487135167169, 23.7805570602317, 6.045478485882434, 55.23228498942336, 14.532942645938238, 0.0, 204.81266924896306, 3.200370809065267, 13.9530353573557, 3.9518174655512666, 3.6306583161525396, 198.96049713829947, 12.043890230072716, 7.050460560967821, 113.3508397408044, 29.19508834126415, 16.45619221422863, 14.62742365835978, 137.6410686538596, 31.9516890461622, 13.03466388593837, 73.39252159700739, 144.83306552755096, 281.31149916089487, 44.4478366474658, 11.068157392406283, 24.62477329364995, 134.2565635511974, 8.254729960267234, 202.67536620921285, 15.398177625678002, 31.479956262134234, 19.316729370816212, 9.760101150139192, 5.481254110940584, 7.272396709240488, 1.5207043902735748, 41.20992334954257, 343.2635302527948, 30.595788152688538, 22.537291750672384, 20.330319282823783, 36.53070308041228, 8.258405508356473, 27.742528782873613, 15.260334725278186, 53.88782253246435, 3.1456897615900794, 18.590492125183726, 19.65264306841382, 16.025641025641026, 15.518881831159337, 9.567983786616566, 32.18211051789313, 14.031477280699702, 24.771661219564816, 7.7370222895282295, 139.92157670580704, 57.12363236211477, 9.806660208138933, 7.59509694297348, 26.42843321865281, 130.1400644146506, 13.95733772629071, 60.63685081695865, 47.285041063924815, 12.503751125337603, 7.518080495404402, 10.4605517422009, 25.58002711482874, 47.56108004942682, 23.79777350550173, 1.5942834717770948, 22.157215998565054, 7.689121099335028, 11.344156498941212, 5.148752747651707, 14.556646148727333, 4.986039927603372, 21.704529011720446, 5.700552269503869, 0.612713185894117, 1.4628042916660255, 12.860270588007186, 14.049755213369817, 36.49663284327822, 102.49612773471475, 5.2161420934531675, 4.426808645404636, 28.642091267487, 25.079896514990107, 30.745917930360495, 20.270632061255554, 75.41995200548509, 107.0312828343677, 2.797549719452389, 0.862099931750422, 2.7767420089331756, 3.417479267292445, 29.55715668719961, 1.848561241678586, 1.1766966985812959, 17.69869997737052, 30.79300711029795, 0.0, 38.080528334043464, 5.303572840236706, 149.18919538057563, 3.1805098357266672, 24.210206053097863, 23.979100320983395, 27.6629480834085, 174.3350531250736, 18.306698217470014, 10.606107870355928, 7.340160870671355, 8.513737862011228, 22.63500686351821, 12.654991601793327, 13.10088833216498, 29.430881626498195, 9.251864981204106, 200.59099744287923, 5.349906835193827, 35.21386744818734, 9.400092181549136, 6.404617810512492, 23.657533544411102, 194.55440176125634, 27.02181046130091, 7.397572602482603, 40.53828726501179, 4.424049131076532, 52.11905308755198, 25.312415739920432, 32.36182755051654, 5.751635689225016, 20.292408659414193, 0.0, 40.23823257559437, 41.370583081865085, 0.0, 0.0, 8.84622293292132, 40.06371907009445, 6.384018418922818, 9.850355339457778, 24.885036316599873, 23.565275056609533, 12.443593192060392, 51.94365678353536, 2.873352565148665, 11.392845293155899, 10.290305345022064, 14.826267910661146, 0.20227513290198645, 30.695576029848475, 2.0030904824586506, 10.111497314133526, 2.617835311980523, 10.36151581394761, 6.987976569725619, 3.957877267590717, 47.06892812953097, 29.2621157057407, 11.354797685892231, 180.73121963226592, 13.182341183794243, 106.87323124802285, 20.838798410367424, 114.71476144058552]\n",
      "567\n",
      "1701\n"
     ]
    }
   ],
   "source": [
    "print(len(num_comments_test))\n",
    "print(len(num_comments_train))\n",
    "print(num_comments_test)\n",
    "print(len(video_captions_test))\n",
    "print(len(video_captions_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build the document-term matrices\n",
    "\n",
    "Use `sklearn.feature_extraction.TfidfVectorizer`. Use unigrams only, disable idf, use `l1` normalization. \n",
    "\n",
    "Resulting matrices are `X_train` and `X_test`.\n",
    "\n",
    "**Note:** Remember to just `fit` on the training data. If a word occurs only in the test documents, our model should **not** be aware that the word exists, as we are trying to evaluate the performance on completely unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(ngram_range=(1,2), lowercase=True, strip_accents=\"unicode\", \n",
    "                      stop_words='english', use_idf=False, norm='l1', min_df=1)\n",
    "tfv.fit(video_captions_train)\n",
    "X_train = tfv.transform(video_captions_train)\n",
    "X_test  = tfv.transform(video_captions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1701, 529663)\n",
      "(567, 529663)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Predict using a random guess baseline\n",
    "\n",
    "Use a random classifier from `sklearn.dummy.DummyClassifier`.  Set `strategy=\"stratified\"`? Set `random_state=0`, to get the same result every time, since randomness is involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dummy = DummyClassifier(strategy=\"stratified\", random_state=0)\n",
    "dummy.fit(X_train, num_comments_train)\n",
    "num_comments_pred_stratified = dummy.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluate the randomized predictions\n",
    "\n",
    "We will use a regression evaluation statistic called mean absolute error initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_mae_score = mean_absolute_error(num_comments_test, num_comments_pred_stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.4873522962\n"
     ]
    }
   ],
   "source": [
    "print(my_mae_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train and evaluate SVM Regression.\n",
    "\n",
    "We will use `sklearn.svm.SVR()\" as our initial classifier (Support Vector Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm_regression_classifier = SVR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.0,\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_regression_classifier.fit(X_train, num_comments_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_comments_pred_svr = svm_regression_classifier.predict(X_test)\n",
    "my_mae_score_svr = mean_absolute_error(num_comments_test, num_comments_pred_svr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.5918421221\n"
     ]
    }
   ],
   "source": [
    "print(my_mae_score_svr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Train and evaluate Linear Regression.\n",
    "\n",
    "We will be using `sklearn.linear_model.LinearRegression()\" as an additional classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_regression_classifier = LinearRegression(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_regression_classifier.fit(X_train, num_comments_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_comments_pred_linreg = linear_regression_classifier.predict(X_test)\n",
    "mae_score_linreg = mean_absolute_error(num_comments_test, num_comments_pred_linreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.8014335828\n"
     ]
    }
   ],
   "source": [
    "print(mae_score_linreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Train and evaluate Lasso Regression.\n",
    "\n",
    "We will be using \"sklearn.linear_model.Lasso\" as an additional classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lasso_classifier = Lasso()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=None,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_classifier.fit(X_train, num_comments_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_comments_pred_lasso = lasso_classifier.predict(X_test)\n",
    "mae_score_lasso = mean_absolute_error(num_comments_test, num_comments_pred_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.3333003983\n"
     ]
    }
   ],
   "source": [
    "print(mae_score_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Train and evaluate KNN Regression.\n",
    "\n",
    "We will be using \"sklearn.neighbors.KNeighborsRegressor\" as an additional classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_regression_classifier = KNeighborsRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "          metric_params=None, n_neighbors=5, p=2, weights='uniform')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_regression_classifier.fit(X_train, num_comments_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_comments_pred_knn = knn_regression_classifier.predict(X_test)\n",
    "mae_score_knn = mean_absolute_error(num_comments_test, num_comments_pred_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.3640198698\n"
     ]
    }
   ],
   "source": [
    "print(mae_score_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Create a query search\n",
    "\n",
    "Return the top ten search results for a given query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Key and version data \n",
    "DEVELOPER_KEY = \"AIzaSyBEuuLWPO0AJIIp7TVGIB1uM_mNiNkMVbw\"\n",
    "YOUTUBE_READ_WRITE_SCOPE = \"https://www.googleapis.com/auth/youtube\"\n",
    "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
    "YOUTUBE_API_VERSION = \"v3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Authenticate \n",
    "youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=DEVELOPER_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Returns the top ten search results for the query in the form \n",
    "   {video_id:{\"thumbnail\":thumbnail_url, \"title\":video title}}\"\"\"\n",
    "\n",
    "def query_search(query):\n",
    "    search_request = youtube.search().list(part=\"snippet\", q=query, maxResults=10, videoCaption=\"closedCaption\", type=\"video\")\n",
    "    \n",
    "    search_response = search_request.execute()\n",
    "    \n",
    "    search_results = {}\n",
    "    for search_result in search_response[\"items\"]:\n",
    "        video_id = search_result[\"id\"][\"videoId\"]\n",
    "        thumbnail = search_result[\"snippet\"][\"thumbnails\"][\"default\"][\"url\"]\n",
    "        title = search_result[\"snippet\"][\"title\"]\n",
    "        description = search_result[\"snippet\"][\"description\"]\n",
    "        search_results[video_id] = {\"thumbnail\":thumbnail, \"title\":title, \"description\":description}\n",
    "    \n",
    "    return search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup_search_results = query_search(\"soup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "import urllib3\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def has_english(vid_id):\n",
    "    http = urllib3.PoolManager() #init urllib\n",
    "    resp = http.request('GET', 'http://video.google.com/timedtext',preload_content=False,\n",
    "                       fields={'type': 'list', 'v': vid_id})\n",
    "    sub_dir_xml = resp.read()\n",
    "    resp.close()\n",
    "    dir_soup = BeautifulSoup(sub_dir_xml)\n",
    "    eng_track = dir_soup.find(lang_code=\"en\")\n",
    "    return False if eng_track is None else True\n",
    "\n",
    "def get_transcript(vid_id):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        vid_id: Youtube video id\n",
    "    Output:\n",
    "        transcript: Beautiful soup xml object of transcipt\n",
    "        of the format:\n",
    "        <transcript>\n",
    "            <text dur=\"DURATION_TIME\" start=\"START_TIME\">\n",
    "                SPOKEN TEXT\n",
    "            </text>\n",
    "        </transcript>\n",
    "    \"\"\"\n",
    "    http = urllib3.PoolManager() #init urllib\n",
    "    resp = http.request('GET', 'http://video.google.com/timedtext',preload_content=False,\n",
    "                       fields={'type': 'list', 'v': vid_id})\n",
    "    sub_dir_xml = resp.read()\n",
    "    resp.close()\n",
    "    dir_soup = BeautifulSoup(sub_dir_xml)\n",
    "    eng_track = dir_soup.find(lang_code=\"en\")\n",
    "    if eng_track is None:\n",
    "        return None\n",
    "    track_resp = http.request('GET', 'http://video.google.com/timedtext',preload_content=False,\n",
    "                               fields={'type': 'track',\n",
    "                                       'v':    vid_id, \n",
    "                                       'name': eng_track['name'].encode('unicode-escape'), \n",
    "                                       'lang': 'en'})\n",
    "    transcript_xml = track_resp.read()\n",
    "    track_resp.close()\n",
    "    return BeautifulSoup(transcript_xml).transcript\n",
    "\n",
    "def get_tokens(text):\n",
    "    text = re.sub(\"&#39;\", \"\\'\", text)\n",
    "    text = re.sub(\"\\n\", \" \", text)\n",
    "    text = re.sub(\"[:&%$#@!,.?]\", \"\", text)\n",
    "    return nltk.word_tokenize(text.lower())\n",
    "\n",
    "def format_transcript(transcript):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        beautifulsoup transcript\n",
    "    Outputs:\n",
    "        array/dictionary formatted transcript\n",
    "    \"\"\"\n",
    "    foramtted_transcript = []\n",
    "    for text_soup in transcript.find_all(\"text\"):\n",
    "        text = text_soup.get_text()\n",
    "        if len(text) > 0:\n",
    "            line = {\n",
    "                    'text'  : text,\n",
    "                    'dur'   : text_soup['dur'] if 'dur' in text_soup else 0,\n",
    "                    'start' : text_soup['start'] if 'start' in text_soup else 0\n",
    "                    }\n",
    "            foramtted_transcript.append(line)\n",
    "    return foramtted_transcript\n",
    "\n",
    "\n",
    "def get_flattened_transcript(vid_id):\n",
    "    transcript = get_transcript(vid_id)\n",
    "    flat_text = \"\"\n",
    "    if transcript is not None:\n",
    "        for text_soup in transcript.find_all(\"text\"):\n",
    "            text = text_soup.get_text()\n",
    "            if len(text) > 0:\n",
    "                flat_text += (text + \" \")\n",
    "    else:\n",
    "        return None # The case where we could not get the transcript\n",
    "    return flat_text[:-1]\n",
    "\n",
    "\n",
    "def get_formatted_transcript(vid_id):\n",
    "    \"\"\"\n",
    "    Convience method\n",
    "    \"\"\"\n",
    "    transcript = get_transcript(vid_id)\n",
    "    if transcript is None:\n",
    "        return None\n",
    "    return format_transcript(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Assign conversationality scores to the videos returned and \n",
    "return a list with the video in ascending order of conversationality\n",
    "score. The final list consists tuple of the form (video_info dictionary, score).\"\"\"\n",
    "\n",
    "def rerank_search_results(model, search_results):\n",
    "    videos_with_score = [] # contain tuples of video dictionaries and their conversationality score\n",
    "    for video_id, video_info in search_results.iteritems():\n",
    "        flattened_transcript = get_flattened_transcript(video_id)\n",
    "        if flattened_transcript is not None: \n",
    "            vectorized_captions = tfv.transform([flattened_transcript]) # using previous vectorizer\n",
    "            conversationality_score = model.predict(vectorized_captions)\n",
    "            videos_with_score.append(({video_id : video_info}, conversationality_score[0]))\n",
    "        \n",
    "    return sorted(videos_with_score, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({u'9QiqibXv9Eo': {'description': u'All videos come with english captions. Please click the CC Button to activate english subtitles. \\u6240\\u6709\\u89c6\\u9891\\u90fd\\u5beb\\u4e0a\\u6709\\u82f1\\u6587\\u8aaa\\u660e\\u5b57\\u5e55, \\u8acb\\u6309CC \\u9215\\u63a3Please add me as ...',\n",
       "    'thumbnail': u'https://i.ytimg.com/vi/9QiqibXv9Eo/default.jpg',\n",
       "    'title': u'Chinese hot and sour soup, \\u9178\\u8fa3\\u6e6f'}},\n",
       "  16.244858248222585),\n",
       " ({u'mZyR2Ew66w8': {'description': u\"Magic Diet Soup - Lose Weight Fast - Low Gi. Well it worked for me and I have lost the weight and kept it off. So don't be scared, just try and see how I did it by ...\",\n",
       "    'thumbnail': u'https://i.ytimg.com/vi/mZyR2Ew66w8/default.jpg',\n",
       "    'title': u'Magic Diet Soup -  Lose Weight Fast - Low Gi.'}},\n",
       "  16.244858041687017),\n",
       " ({u'vZwfONmD54c': {'description': u'Sweet Corn Soup,delicate and pleasant flavour of corn makes this a popular choice. Recipe link : http://www.tarladalal.com/Sweet-Corn-Soup-(-Soup-)-37393r ...',\n",
       "    'thumbnail': u'https://i.ytimg.com/vi/vZwfONmD54c/default.jpg',\n",
       "    'title': u'Sweet Corn Soup by Tarla Dalal'}},\n",
       "  16.244858033535834),\n",
       " ({u'O9ak89FwYeI': {'description': u'Twitter: http://www.twitter.com/tweetsauce Instagram: http://www.instagram.com/electricpants ***EXTRA INFO AND SOURCES BELOW*** Vsauce2: ...',\n",
       "    'thumbnail': u'https://i.ytimg.com/vi/O9ak89FwYeI/default.jpg',\n",
       "    'title': u'Is Cereal Soup?'}},\n",
       "  16.244857525236686),\n",
       " ({u'GojmNjoTaTg': {'description': u'LIVE EVENT WITH EMERIL & LAURA: https://www.youtube.com/watch?v=lnDSYdUp1hA To get this complete recipe with instructions and measurements, check ...',\n",
       "    'thumbnail': u'https://i.ytimg.com/vi/GojmNjoTaTg/default.jpg',\n",
       "    'title': u'Loaded Potato Soup Recipe - Laura Vitale - Laura in the Kitchen Episode 863'}},\n",
       "  16.244857073823216),\n",
       " ({u'1xrhaL3WmaY': {'description': u\"If you haven't got a failsafe soup recipe in your arsenal than look no further than this tasty offering from Vegetarian cook Anna Jones. From one batch of sweet potato and ...\",\n",
       "    'thumbnail': u'https://i.ytimg.com/vi/1xrhaL3WmaY/default.jpg',\n",
       "    'title': u'Easy Vegetable Soup - Three Ways | Anna Jones'}},\n",
       "  16.244856752269577),\n",
       " ({u'L1TFnkm1TG8': {'description': u'Click here to SUBSCRIBE: http://bit.ly/1dn24vP Shop this video here: https://rdy.cr/dac6c6 Homemade Vegetable Stock: http://bit.ly/1vAte5U 3 Mac & Cheese ...',\n",
       "    'thumbnail': u'https://i.ytimg.com/vi/L1TFnkm1TG8/default.jpg',\n",
       "    'title': u'Fall Soup - 3 Delicious Ways'}},\n",
       "  16.244856654118454),\n",
       " ({u'xqFo59YveXo': {'description': u'Food Busker is hitting the streets with another incredible recipe: Ramen noodles layered with finely sliced fresh vegetables and shredded chicken in an Asian ...',\n",
       "    'thumbnail': u'https://i.ytimg.com/vi/xqFo59YveXo/default.jpg',\n",
       "    'title': u'Chicken Ramen Noodle Soup | Food Busker'}},\n",
       "  16.24485657861317)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rerank_search_results(svm_regression_classifier, soup_search_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Use grid search and cross-validation to tune the classifier\n",
    "\n",
    "The score above is pretty disappointing, but kind of expected, given how little work we did-- we are basically just using the default configuration.  A `LinearSVC` has a bunch of configuration options that should be tweaked:\n",
    "\n",
    "* `C` is the *regularization parameter*. Lower values of C constraint the model more, while higher values allows the model to fit the training data better. (Remember that fitting the training data too well can lead to overfitting.)\n",
    "\n",
    "* `class_weight` can force the classifier to emphasize positive instances more or less than negative ones. This is useful if we know for a fact that the classes aren't equally probable. Read the documentation and see what the `'auto'` setting does.\n",
    "\n",
    "However, choosing these values should also be done without looking at the test data, because they are part of the model. Use `sklearn.grid_search.GridSearchCV` to systematically try out different values for these two parameters, and choose the configuration that does best.\n",
    "\n",
    "`GridSearchCV` uses k-fold cross-validation to ensure fair evaluation and avoid overfitting. This consists of splitting the training data into *k* parts, then training the classifier *k* times, each time leaving out a different part, that is used for scoring. The average score over the *k* folds is a better estimate of how well the classifier would generalize.\n",
    "\n",
    "Because we are facing a multi-label problem, the default scoring strategy (accuracy) doesn't make sense. We have to define our own `sample_f1_scorer` strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_mae_scorer(estimator, X, y):\n",
    "    \"\"\"sample-mae scorer metric\n",
    "    \n",
    "    This function is just glue code for the scikit-learn scorer API.\n",
    "    See http://scikit-learn.org/stable/modules/model_evaluation.html#implementing-your-own-scoring-object\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    \n",
    "    estimator:\n",
    "        the model that should be evaluated (e.g., the scikit-learn classifier)\n",
    "    X: array-like, shape (n_samples, n_features)\n",
    "        the test data\n",
    "    y: array-like, shape (n_samples, n_labels)\n",
    "        the ground truth target for X.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    \n",
    "    mean_absolute_error, float\n",
    "    \"\"\"\n",
    "    y_pred = estimator.predict(X)\n",
    "    return mean_absolute_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run grid search over a range of regularization parameters, as below.  This takes under 1 minute on a 2014 MacBook Pro Retina. If you're not sure your code works, test it on a small number of documents first to avoid wasting time.\n",
    "\n",
    "What is the best configuration, and the best score (averaged over the 3 folds)? (there are attributes of the `GridSearchCV` object that answer this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'C':[1, 10, 100, 1000]}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(svm_regression_classifier,\n",
    "                    param_grid,\n",
    "                    cv=3,\n",
    "                    scoring=sample_mae_scorer,\n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:   11.2s\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.0,\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False),\n",
       "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
       "       param_grid=[{'C': [1, 10, 100, 1000]}], pre_dispatch='2*n_jobs',\n",
       "       refit=True, score_func=None,\n",
       "       scoring=<function sample_mae_scorer at 0x1AB3A070>, verbose=True)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, num_comments_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Evaluate the chosen classifier on the test set. Inspect performance on individual categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_mae_score = sample_mae_scorer(grid.best_estimator_, X_test, num_comments_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.5918421221\n"
     ]
    }
   ],
   "source": [
    "print(grid_mae_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Then, to aggregate scores over individual categories, use `sklearn.metrics.classification_report`. Keep in mind that in the classification report, precision, recall and F1 have different meaning than the sample-based scores we used in the previous questions: they are averages over a given label, as opposed to a given document.\n",
    "\n",
    "DISCUSSION ITEM. How do you interpret this table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "not all arguments converted during string formatting",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-534d33e397ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mY_pred_test_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgrid_report\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_comments_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_pred_test_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\sklearn\\metrics\\classification.pyc\u001b[0m in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits)\u001b[0m\n\u001b[0;32m   1222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1224\u001b[1;33m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python27\\lib\\site-packages\\sklearn\\utils\\multiclass.pyc\u001b[0m in \u001b[0;36munique_labels\u001b[1;34m(*ys)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[0m_unique_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FN_UNIQUE_LABELS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_unique_labels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unknown label type: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[0mys_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_unique_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: not all arguments converted during string formatting"
     ]
    }
   ],
   "source": [
    "Y_pred_test_grid = grid.predict(X_test)\n",
    "grid_report = classification_report(num_comments_test, Y_pred_test_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(grid_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
