{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube Conversation Prediction\n",
    "## CS/INFO 4300 Language and Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the data from the JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data.json') as json_file:   \n",
    "    video_data = json.load(json_file)\n",
    "video_num_comments, video_captions = np.array([ (video_data[video_id][\"numberComments\"], video_data[video_id][\"captions\"]) \n",
    "                                  for video_id in videos_data.keys() ]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_video_captions = []\n",
    "\n",
    "caption_text = \"\"\n",
    "for caption_data_list in video_captions:\n",
    "    for caption_data in caption_data_list:\n",
    "        # Consolidate caption text for each video into one string\n",
    "        caption_text = caption_text + caption_data[\"text\"] + \" \"\n",
    "    combined_video_captions.append(caption_text)\n",
    "    caption_text = \"\"\n",
    "    \n",
    "video_captions = combined_video_captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2. Make a 50-50 train-test split.\n",
    "\n",
    "Use `sklearn.cross_validation.train_test_split`. Set `random_state=0`. Make sure the train and test sizes are equal (plus/minus one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(video_num_comments))\n",
    "print(len(video_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_comments_train, num_comments_test, video_captions_train, video_captions_test  = train_test_split(video_num_comments, video_captions, \n",
    "                                                                       test_size=.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n",
      "[9791 133980 17250]\n",
      "3\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(num_comments_test))\n",
    "print(len(num_comments_train))\n",
    "print(num_comments_test)\n",
    "print(len(video_captions_test))\n",
    "print(len(video_captions_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build the document-term matrices\n",
    "\n",
    "Use `sklearn.feature_extraction.TfidfVectorizer`. Use unigrams only, disable idf, use `l1` normalization. \n",
    "\n",
    "Resulting matrices are `X_train` and `X_test`.\n",
    "\n",
    "**Note:** Remember to just `fit` on the training data. If a word occurs only in the test documents, our model should **not** be aware that the word exists, as we are trying to evaluate the performance on completely unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(ngram_range=(1,1), use_idf=False, norm='l1')\n",
    "tfv.fit(video_captions_train)\n",
    "X_train = tfv.transform(video_captions_train)\n",
    "X_test  = tfv.transform(video_captions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 531)\n",
      "(3, 531)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Predict using a random guess baseline\n",
    "\n",
    "Use a random classifier from `sklearn.dummy.DummyClassifier`.  Set `strategy=\"stratified\"`? Set `random_state=0`, to get the same result every time, since randomness is involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dummy = DummyClassifier(strategy=\"stratified\", random_state=0)\n",
    "dummy.fit(X_train, num_comments_train)\n",
    "num_comments_pred_stratified = dummy.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluate the randomized predictions\n",
    "\n",
    "We will use a regression evaluation statistic called mean absolute error initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_mae_score = mean_absolute_error(num_comments_test, Y_pred_stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42995.6666667\n"
     ]
    }
   ],
   "source": [
    "print(my_mae_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train and evaluate a classifier.\n",
    "\n",
    "We will use `sklearn.svm.SVR()\" as our initial classifier (Support Vector Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm_regression_classifier = SVR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.0,\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_regression_classifier.fit(X_test, num_comments_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_comments_pred_svr = svm_regression_classifier.predict(X_test)\n",
    "my_mae_score_svr = mean_absolute_error(num_comments_test, num_comments_pred_svr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41396.333308\n"
     ]
    }
   ],
   "source": [
    "print(my_mae_score_svr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Use grid search and cross-validation to tune the classifier\n",
    "\n",
    "The score above is pretty disappointing, but kind of expected, given how little work we did-- we are basically just using the default configuration.  A `LinearSVC` has a bunch of configuration options that should be tweaked:\n",
    "\n",
    "* `C` is the *regularization parameter*. Lower values of C constraint the model more, while higher values allows the model to fit the training data better. (Remember that fitting the training data too well can lead to overfitting.)\n",
    "\n",
    "* `class_weight` can force the classifier to emphasize positive instances more or less than negative ones. This is useful if we know for a fact that the classes aren't equally probable. Read the documentation and see what the `'auto'` setting does.\n",
    "\n",
    "However, choosing these values should also be done without looking at the test data, because they are part of the model. Use `sklearn.grid_search.GridSearchCV` to systematically try out different values for these two parameters, and choose the configuration that does best.\n",
    "\n",
    "`GridSearchCV` uses k-fold cross-validation to ensure fair evaluation and avoid overfitting. This consists of splitting the training data into *k* parts, then training the classifier *k* times, each time leaving out a different part, that is used for scoring. The average score over the *k* folds is a better estimate of how well the classifier would generalize.\n",
    "\n",
    "Because we are facing a multi-label problem, the default scoring strategy (accuracy) doesn't make sense. We have to define our own `sample_f1_scorer` strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_f1_scorer(estimator, X, y):\n",
    "    \"\"\"sample-f1 scorer metric\n",
    "    \n",
    "    This function is just glue code for the scikit-learn scorer API.\n",
    "    See http://scikit-learn.org/stable/modules/model_evaluation.html#implementing-your-own-scoring-object\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    \n",
    "    estimator:\n",
    "        the model that should be evaluated (e.g., the scikit-learn classifier)\n",
    "    X: array-like, shape (n_samples, n_features)\n",
    "        the test data\n",
    "    y: array-like, shape (n_samples, n_labels)\n",
    "        the ground truth target for X.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    \n",
    "    sample_f1_score, float\n",
    "        the sample F1 score as used in Q06 and Q07\n",
    "    \"\"\"\n",
    "    y_pred = estimator.predict(X)\n",
    "    return f1_score(y, y_pred, average='samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run grid search over a range of regularization parameters, as below.  This takes under 1 minute on a 2014 MacBook Pro Retina. If you're not sure your code works, test it on a small number of documents first to avoid wasting time.\n",
    "\n",
    "What is the best configuration, and the best score (averaged over the 3 folds)? (there are attributes of the `GridSearchCV` object that answer this).\n",
    "\n",
    "DISCUSSION ITEM.\n",
    "What can you say about the impact of `C` and `class_weight` on the score? (look at `grid.grid_scores_` to answer this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = dict(\n",
    "    estimator__C=[1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3],  # you can also build this using np.logspace\n",
    "    estimator__class_weight=['auto', None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(ovr,\n",
    "                    param_grid,\n",
    "                    cv=3,\n",
    "                    scoring=sample_f1_scorer,\n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done  42 out of  42 | elapsed:   28.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 14 candidates, totalling 42 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "       estimator=OneVsRestClassifier(estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',\n",
       "     random_state=None, tol=0.0001, verbose=0),\n",
       "          n_jobs=1),\n",
       "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
       "       param_grid={'estimator__class_weight': ['auto', None], 'estimator__C': [0.001, 0.01, 0.1, 1, 10.0, 100.0, 1000.0]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, score_func=None,\n",
       "       scoring=<function sample_f1_scorer at 0x7feeaf225500>, verbose=True)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Evaluate the chosen classifier on the test set. Inspect performance on individual categories.\n",
    "\n",
    "Use `grid.best_estimator_` to access the `ovr` object chosen as best by the grid search. Use `sample_f1_scorer` and report the **sample F1** score as in Q06 and Q07. This time, you should see a rewarding increase.\n",
    "\n",
    "DISCUSSION ITEM.\n",
    "Compare this score with the cross-validated average score over the 3 folds for the best model (Q08).  Does cross-validation give a reasonable estimate of the actual generalization performance a model can get on unseen test data? Compare with what we saw in class, when we were looking at the performance of a classifier on the data it was trained on, versus on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_f1_score = sample_f1_scorer(grid.best_estimator_, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.563988708163\n"
     ]
    }
   ],
   "source": [
    "print(grid_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** TODO discuss **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Then, to aggregate scores over individual categories, use `sklearn.metrics.classification_report`. Keep in mind that in the classification report, precision, recall and F1 have different meaning than the sample-based scores we used in the previous questions: they are averages over a given label, as opposed to a given document.\n",
    "\n",
    "DISCUSSION ITEM. How do you interpret this table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_pred_test_grid = grid.predict(X_test)\n",
    "grid_report = classification_report(Y_test, Y_pred_test_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.73      0.65        78\n",
      "          1       0.49      0.57      0.53        54\n",
      "          2       0.61      0.66      0.64        89\n",
      "          3       0.49      0.73      0.59        52\n",
      "          4       0.69      0.65      0.67       156\n",
      "          5       0.39      0.41      0.40        39\n",
      "          6       0.64      0.49      0.56        55\n",
      "          7       0.29      0.39      0.33        46\n",
      "          8       0.36      0.46      0.41        68\n",
      "          9       0.68      0.67      0.68        61\n",
      "         10       0.66      0.72      0.69       120\n",
      "\n",
      "avg / total       0.57      0.62      0.59       818\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(grid_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
